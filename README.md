# üß† Deep Learning Assignment: MNIST Classification with Multiple Optimizers & Activations

This project implements a fully-connected neural network **from scratch using NumPy** for classifying handwritten digits from the **MNIST** dataset. The network is trained using various **activation functions** and **optimizers**, allowing a comparative analysis of their impact on performance.

## üìå Features

* ‚úÖ Manual implementation of neural networks (no TensorFlow/PyTorch)
* ‚úÖ Supports multiple **activation functions**: `ReLU`, `Sigmoid`, `Tanh`, `Leaky ReLU`
* ‚úÖ Includes popular **optimizers**: `SGD`, `Mini-Batch SGD`, `Momentum`, `Adam`, `Adagrad`, `Adadelta`
* ‚úÖ Visual comparison of accuracy trends
* ‚úÖ Evaluation on **MNIST** dataset (70k grayscale images)

---

## üìä Activation Functions & Optimizers Compared

| Activation Function | Optimizers Tested                                    |
| ------------------- | ---------------------------------------------------- |
| ReLU                | SGD, MiniBatchSGD, Momentum, Adam, Adagrad, Adadelta |
| Sigmoid             | SGD, MiniBatchSGD, Momentum, Adam, Adagrad, Adadelta |
| Tanh                | SGD, MiniBatchSGD, Momentum, Adam, Adagrad, Adadelta |
| Leaky ReLU          | SGD, MiniBatchSGD, Momentum, Adam, Adagrad, Adadelta |

Each combination is evaluated over 10 epochs using a network with the architecture:

```
Input Layer (784) ‚Üí Hidden Layer 1 (128) ‚Üí Hidden Layer 2 (64) ‚Üí Output Layer (10)
```

---

## üìà Visual Results

Two sets of plots are generated:

1. **Optimizers Comparison (for each Activation)**
2. **Activations Comparison (for each Optimizer)**

These allow you to observe the effect of optimizers and activation functions independently.

---

## üß™ Final Test Accuracy

| Activation + Optimizer                          | Accuracy |
| ----------------------------------------------- | -------- |
| ReLU + Adam                                     | \~0.97 ‚úÖ |
| Tanh + Adam                                     | \~0.96   |
| Leaky ReLU + Adam                               | \~0.965  |
| Sigmoid + SGD                                   | \~0.92   |


---

## üß¨ Project Structure

```
‚îú‚îÄ‚îÄ neural_network.py        # Main NN architecture and training logic
‚îú‚îÄ‚îÄ activations.py           # Activation functions and their derivatives
‚îú‚îÄ‚îÄ optimizers.py            # SGD, Adam, Adagrad, Adadelta, etc.
‚îú‚îÄ‚îÄ run_experiments.py       # Training loop for all combinations
‚îú‚îÄ‚îÄ visualizations.py        # Accuracy plots
‚îî‚îÄ‚îÄ README.md                # Project overview
```

---

## ‚öôÔ∏è Requirements

Install the required Python packages:

```bash
pip install numpy matplotlib scikit-learn
```

---

## üöÄ How to Run

```bash
python run_experiments.py
```

This script:

* Trains 24 combinations (4 activations √ó 6 optimizers)
* Stores and plots training accuracy over epochs
* Prints test set accuracy for each trained model

---

## üß† What You Learn

* How activation functions affect gradient flow and learning
* How different optimizers influence convergence
* How to implement a neural network from scratch (forward + backward pass)
* Practical deep learning experimentation without relying on high-level libraries

---

## üìö Dataset Used

* [MNIST Handwritten Digits](https://www.openml.org/d/554)
* 70,000 grayscale 28√ó28 images (10 classes: 0‚Äì9)

---

## üìå Future Improvements

* Add dropout & batch normalization
* Add model saving/loading
* Add learning rate scheduler
* Extend to other datasets (e.g., Fashion-MNIST)

---

## üßë‚Äçüíª Author

**Swapnil Halder**
üîó [LinkedIn](https://www.linkedin.com/in/swapnilhalder/)

---

> ‚≠ê Star this repository if it helped you learn or if you like the clean from-scratch implementation!

---

